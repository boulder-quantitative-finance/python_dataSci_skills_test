{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Data-I/O\" data-toc-modified-id=\"Data-I/O-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data I/O</a></span><ul class=\"toc-item\"><li><span><a href=\"#[a]\" data-toc-modified-id=\"[a]-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>[a]</a></span></li><li><span><a href=\"#[b]\" data-toc-modified-id=\"[b]-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>[b]</a></span></li></ul></li><li><span><a href=\"#Data-Cleaning/Processing\" data-toc-modified-id=\"Data-Cleaning/Processing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Cleaning/Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#[a]\" data-toc-modified-id=\"[a]-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>[a]</a></span></li><li><span><a href=\"#[b]\" data-toc-modified-id=\"[b]-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>[b]</a></span></li><li><span><a href=\"#[c]\" data-toc-modified-id=\"[c]-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>[c]</a></span></li><li><span><a href=\"#[d]\" data-toc-modified-id=\"[d]-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>[d]</a></span></li><li><span><a href=\"#[e]\" data-toc-modified-id=\"[e]-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>[e]</a></span></li></ul></li><li><span><a href=\"#Data-Exploration\" data-toc-modified-id=\"Data-Exploration-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data Exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#[a]\" data-toc-modified-id=\"[a]-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>[a]</a></span></li></ul></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Feature Engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#[a]\" data-toc-modified-id=\"[a]-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>[a]</a></span></li></ul></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#[a]\" data-toc-modified-id=\"[a]-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>[a]</a></span></li></ul></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#[a]\" data-toc-modified-id=\"[a]-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>[a]</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This is intended to be a demo of your skills on a fairly common scientific python workflow. These challenges are meant to act as a baseline assessment of how you apply your skills to real-world use cases.  I don't expect a profitable system from this notebook so breathe easy. \n",
    "\n",
    "- `Data I/O:` Importing and saving datasets in a structured way\n",
    "- `Data Cleaning/Processing:` Refining datasets for use in scientific analysis\n",
    "- `Data Exploration:` Learning about the dataset\n",
    "- `Feature Engineering:` Finding useful predictors\n",
    "- `Modeling:` Testing predictors\n",
    "- `Evaluation:` Interpreting results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data I/O\n",
    "\n",
    "[Raw Options Data Download Link](https://drive.google.com/file/d/1ZZtVkDrLo7LysEQCrEyKpsmiGFivyuPl/view?usp=sharing)\n",
    "\n",
    "This is an `hourly options dataset` covering a period of one month stored as a `parquet` file. The dataset is dirty.\n",
    "\n",
    "### [a]\n",
    "`Download` the dataset to the `./data/raw/` folder.\n",
    "\n",
    "### [b]\n",
    "`Import` the dataset. If you have difficulty try using `dask.dataframe` which lets users operate on data that is larger than memory. Below are some common package imports. Feel free to add your own as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T22:40:28.700905Z",
     "start_time": "2018-04-11T22:40:28.578815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "2018-06-07T16:42:22-06:00\n",
      "\n",
      "CPython 3.6.4\n",
      "IPython 6.2.1\n",
      "\n",
      "compiler   : GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)\n",
      "system     : Darwin\n",
      "release    : 17.3.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 8\n",
      "interpreter: 64bit\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\n",
      "project directory: /Users/jnielsen/other-dev/skillz\n",
      "\n",
      "pandas 0.22.0\n",
      "numpy 1.14.3\n",
      "scipy 1.0.1\n",
      "sklearn 0.19.1\n",
      "dask 0.16.1\n",
      "pyarrow 0.9.0\n",
      "fastparquet 0.1.5\n",
      "numba 0.36.2\n",
      "matplotlib 2.1.2\n",
      "seaborn 0.8.1\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import standard libs\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import display\n",
    "from IPython.core.debugger import set_trace as bp\n",
    "from pathlib import PurePath, Path\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# get project dir\n",
    "pp = PurePath(Path.cwd()).parts[:-1]\n",
    "project_dir = PurePath(*pp)\n",
    "print(f'\\nproject directory: {project_dir}')\n",
    "data_dir = project_dir / 'data'\n",
    "script_dir = project_dir / 'src' \n",
    "sys.path.append(script_dir.as_posix())\n",
    "\n",
    "# import python scientific stack\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from numba import jit\n",
    "import math\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "pbar = ProgressBar(); pbar.register()\n",
    "\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# import visual tools\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "blue, green, red, purple, gold, teal = sns.color_palette('colorblind', 6)\n",
    "\n",
    "\n",
    "plt.style.use(['seaborn-talk','bmh'])\n",
    "#mpl.rcParams['font.family'] = 'Bitstream Vera Sans'\n",
    "#mpl.rcParams['font.size'] = 9.5\n",
    "mpl.rcParams['font.weight'] = 'medium'\n",
    "mpl.rcParams['figure.figsize'] = 10,7\n",
    "\n",
    "print()\n",
    "%watermark -p pandas,numpy,scipy,sklearn,dask,pyarrow,fastparquet,numba,matplotlib,seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file size: 414.53 MB\n",
      "number of dask partitions: 1\n",
      "number of dask partitions after repartition: 8\n"
     ]
    }
   ],
   "source": [
    "### Please begin here ###\n",
    "fp = str(data_dir)+'/raw/intraday_options_data_2017-09-13_2017-10-18.parquet'\n",
    "size = round(os.stat(fp).st_size/1000/1000, 2)\n",
    "print('file size: {} MB'.format(size))\n",
    "ddf = dd.read_parquet(fp) # fastparquet is default\n",
    "print('number of dask partitions: {}'.format(ddf.npartitions))\n",
    "ddf = ddf.repartition(npartitions=8)\n",
    "print('number of dask partitions after repartition: {}'.format(ddf.npartitions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning/Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [a]\n",
    "\n",
    "`Clean` the raw options data. Use your preferred methodology to standardize the dataset. Please take care to comment or briefly describe what your code does. Make sure to track how long it takes your code to run.\n",
    "\n",
    "### [b]\n",
    "`Add` the following columns:\n",
    "- `spread`: bid-ask spread\n",
    "- `midquote`: \n",
    "- `spread_pct`: bid-ask spread expressed as pct of price\n",
    "    \n",
    "### [c]\n",
    "`Add` the `calculated` intrinsic value of the options as a column called `intrinsic_value`.\n",
    "    \n",
    "When complete `save` the dataset as a `parquet` file in the `./data/interim/` folder.\n",
    "### [d]\n",
    "`Add` an indicator column to indicate options which are `In-the-money (ITM)`, `At-the-money (ATM)`, `Out-the-money (OTM)`. Use a `5%` corridor to determine if an option is `ITM`. Feel free to use a numeric label to represent the 3 classes.\n",
    "\n",
    "### [e]\n",
    "\n",
    "`Save` cleaned dataset to the `./data/processed/` folder as both `parquet` and `csv`. \n",
    "\n",
    "What is the size difference between the csv file and the parquet file expressed as a ratio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T20:09:08.935997Z",
     "start_time": "2018-04-11T20:09:08.933178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed | 18.0s\n",
      "[########################################] | 100% Completed | 18.1s\n",
      "[########################################] | 100% Completed | 18.0s\n",
      "[########################################] | 100% Completed | 18.0s\n",
      "[########################################] | 100% Completed | 17.5s\n",
      "[########################################] | 100% Completed | 17.5s\n",
      "[########################################] | 100% Completed | 18.1s\n",
      "[########################################] | 100% Completed | 18.2s\n",
      "[########################################] | 100% Completed | 18.0s\n",
      "[########################################] | 100% Completed | 18.1s\n",
      "6.4 : 15789.33\n"
     ]
    }
   ],
   "source": [
    "### Please begin here ###\n",
    "# [a]\n",
    "#  \n",
    "#   define missing data, incorrect data and dupe data\n",
    "#     Missing: NaN or blank\n",
    "#     Incorrect: ?\n",
    "#     Dupe: ?\n",
    "#   handle data issues\n",
    "#     Missing : ffill : none\n",
    "#     Incorrent: drop : none\n",
    "#     Dupe: drop dupe : none\n",
    "#     Case for string cols: none\n",
    "\n",
    "# all column headers to lowercase\n",
    "ddf.columns = [x.lower().strip() for x in ddf.columns]\n",
    "# # all options types to lowercase\n",
    "# ddf.type = df.type.str.lower()\n",
    "\n",
    "# operate on single etf from dd (for faster testing)\n",
    "ddf = ddf[(ddf[\"underlying\"] == \"ACWI\") & (ddf[\"type\"] == \"call\")]\n",
    "\n",
    "# [b]\n",
    "ddf[\"spread\"] = ddf[\"ask\"] - ddf[\"bid\"]\n",
    "ddf[\"midquote\"] = (ddf[\"ask\"] + ddf[\"bid\"]) / 2\n",
    "ddf[\"spread_pct\"] = ddf[\"spread\"] / ddf[\"midquote\"]\n",
    "\n",
    "# add col for Time To Expiration (tte) represented as a fraction of a year\n",
    "ddf = ddf.map_partitions(lambda x: x.assign(\n",
    "    tte = (dd.to_timedelta(x[\"expiry\"]) - dd.to_timedelta(x[\"quote_time\"])).astype('timedelta64[D]') / 365\n",
    "))\n",
    "\n",
    "# [c]\n",
    "ddf = ddf.map_partitions(lambda x: x.assign(\n",
    "    intrinsic_value = np.where(\n",
    "        x[\"type\"]==\"call\", (x[\"underlying_price\"] - x[\"strike\"]), (x[\"strike\"] - x[\"underlying_price\"])\n",
    "    )\n",
    "))\n",
    "\n",
    "ddf.compute()\n",
    "\n",
    "# save to interim dir\n",
    "interim_fp = str(data_dir)+'/interim/intraday_options_data_2017-09-13_2017-10-18.parquet'\n",
    "dd.to_parquet(ddf,interim_fp)\n",
    "\n",
    "# [d]\n",
    "### moneyness ###\n",
    "#    if call\n",
    "#        strike < underlying_price \n",
    "#    if put\n",
    "#        strike > underlying\n",
    "\n",
    "def is_atm(strike, underlying_price, percent_window = .05):\n",
    "    r = strike * (percent_window/2)\n",
    "    lower_threshold = strike - r\n",
    "    upper_threshold = strike + r\n",
    "    return ((lower_threshold <= underlying_price) & (underlying_price <= upper_threshold))\n",
    "\n",
    "ddf = ddf.map_partitions(lambda x: x.assign(\n",
    "    moneyness = np.where(\n",
    "        is_atm(x[\"strike\"], x[\"underlying_price\"]), \"ATM\", np.where(\n",
    "            x[\"type\"]==\"call\", np.where( # call options\n",
    "                (x[\"strike\"] < x[\"underlying_price\"]), \"ITM\", \"OTM\"\n",
    "            ), np.where( # put options\n",
    "                (x[\"strike\"] > x[\"underlying_price\"]), \"ITM\", \"OTM\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "))\n",
    "\n",
    "ddf.compute()\n",
    "\n",
    "processed_fp_parquet = str(data_dir)+'/processed/intraday_options_data_2017-09-13_2017-10-18.parquet'\n",
    "dd.to_parquet(ddf, processed_fp_parquet)\n",
    "\n",
    "processed_fp_csv = str(data_dir)+'/processed/intraday_options_data_2017-09-13_2017-10-18_*.csv'\n",
    "dd.to_csv(ddf, processed_fp_csv)\n",
    "\n",
    "parquet_size = os.path.getsize(processed_fp_parquet)\n",
    "csv_size = 0\n",
    "\n",
    "csv = glob.glob(processed_fp_csv)\n",
    "for f in csv:\n",
    "    csv_size = csv_size + os.path.getsize(f)\n",
    "    \n",
    "print(str(parquet_size/100)+\" : \"+str(csv_size/100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "### [a] \n",
    "\n",
    "Use your favorite methodology to explore and extract interesting facts about the dataset. This is a good section to make use of visuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Please begin here ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Our goal is to find a predictor of the `sign` of `ATM`, `IWM` option price returns.\n",
    "\n",
    "### [a]\n",
    "\n",
    "Use your favorite methodology to discover one or more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Please begin here ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "### [a]\n",
    "\n",
    "Use your favorite methodology to set up a model for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Please begin here ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### [a]\n",
    "\n",
    "Evaluate your model results. Did it turn out how you expected? What would you do differently if given more time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Please begin here ###\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
